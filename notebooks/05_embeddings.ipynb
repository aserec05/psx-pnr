{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 — Embeddings\n",
    "Generate and compare embeddings from 4 models:\n",
    "- Word2Vec (trained on our corpus)\n",
    "- SBERT multilingual\n",
    "- SBERT MiniLM (English)\n",
    "- SBERT MPNet (English, best quality)\n",
    "\n",
    "**Goal**: pick the best model for the recommender based on similarity quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNH-PSX: (8858, 5)\n",
      "Pakistan News: (25912, 5)\n",
      "\n",
      "Available SBERT models: {'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2', 'minilm': 'all-MiniLM-L6-v2', 'mpnet': 'all-mpnet-base-v2'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from embeddings import train_word2vec, embed_corpus, load_sbert, load_embeddings, SBERT_MODELS\n",
    "\n",
    "df_cnhpsx = pd.read_csv('../data/processed/cnhpsx_clean.csv')\n",
    "df_news   = pd.read_csv('../data/processed/pakistan_news_clean.csv')\n",
    "\n",
    "print('CNH-PSX:', df_cnhpsx.shape)\n",
    "print('Pakistan News:', df_news.shape)\n",
    "print('\\nAvailable SBERT models:', SBERT_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus size: 34770 texts\n",
      "[Word2Vec] Tokenizing 34770 texts...\n",
      "[Word2Vec] Training (vector_size=100, window=5, epochs=10)...\n",
      "[Word2Vec] Vocabulary size: 15243 words\n",
      "[Word2Vec] Model saved to ../data/processed/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_corpus = list(df_cnhpsx['headline_clean'].dropna()) + list(df_news['text_clean'].dropna())\n",
    "print(f'Training corpus size: {len(w2v_corpus)} texts')\n",
    "\n",
    "w2v_model = train_word2vec(\n",
    "    texts=w2v_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    epochs=10,\n",
    "    save_path='../data/processed/word2vec.model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'karachi' -> [('depleting', 0.765), ('lahore', 0.765), ('islamabad', 0.735), ('karachis', 0.704)]\n",
      "'bank' -> [('bank’s', 0.792), ('nbp', 0.791), ('fardan', 0.789), ('hbl', 0.788)]\n",
      "'oil' -> [('spiral', 0.864), ('tola', 0.854), ('urea', 0.836), ('offtake', 0.821)]\n",
      "'market' -> [('trading', 0.905), ('gains', 0.884), ('points', 0.881), ('psx', 0.881)]\n",
      "'stock' -> [('investor', 0.823), ('modest', 0.813), ('loss', 0.81), ('buying', 0.81)]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check — similar words\n",
    "for word in ['karachi', 'bank', 'oil', 'market', 'stock']:\n",
    "    if word in w2v_model.wv:\n",
    "        similar = w2v_model.wv.most_similar(word, topn=4)\n",
    "        print(f\"'{word}' -> {[(w, round(s,3)) for w, s in similar]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed_corpus] Embedding 8858 texts with word2vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec: 100%|███████████████████████████████████████████████████████████████████| 8858/8858 [00:01<00:00, 8176.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed_corpus] Done. Shape: (8858, 100)\n",
      "[embed_corpus] Saved to ../data/processed/embeddings_w2v.npy\n"
     ]
    }
   ],
   "source": [
    "w2v_embeddings = embed_corpus(\n",
    "    texts=list(df_cnhpsx['headline_clean']),\n",
    "    method='word2vec',\n",
    "    model=w2v_model,\n",
    "    save_path='../data/processed/embeddings_w2v.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SBERT — 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Loading: paraphrase-multilingual-MiniLM-L12-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07451fe009124b2f9241e19f18bb244b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422de7ca919e4c73b9b514eb331ad580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347fd85e9884484ba02cd21d9b08fcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bec2c49b1c646b98ec7ec47bbfe43a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1c85abdce44ec7ab6d48d3798c96d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb1d35a2cd347b6a792b76df3e1ace6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac73252be5ae4ce595ce86823f511ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b8f39350ab4ae085324a3b0bef0114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/526 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e47b62267647e180ca521834f35c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d87629492b49f4bf482d593395a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d9c9bb38ee4e939d8c00a1feb87c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Ready — embedding dim: 384\n",
      "[embed_corpus] Embedding 8858 texts with sbert...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6903edc720ad40d5bb3840e424ba0117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed_corpus] Done. Shape: (8858, 384)\n",
      "[embed_corpus] Saved to ../data/processed/embeddings_sbert_multilingual.npy\n"
     ]
    }
   ],
   "source": [
    "# SBERT Multilingual\n",
    "sbert_multi = load_sbert('multilingual')\n",
    "emb_multi = embed_corpus(\n",
    "    texts=list(df_cnhpsx['headline_clean']),\n",
    "    method='sbert',\n",
    "    model=sbert_multi,\n",
    "    save_path='../data/processed/embeddings_sbert_multilingual.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Loading: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a294f3cd14281a75e42c335bbf2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f5dfbae1c145a29de6e9638904b7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12244f2440b4e54b49640572e227c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce9f48882ca445eb4c0528edb6a3a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734f80dd27df43b298d4e04276526fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27987f9c7a9e49f2a6bef03ca91b3ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a89ca4da8647be826a43f412668035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e123486d3964c0b8f2f92a10a0417f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf6486d1cff45c18753d63464f74526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b17e2922544356b54658fd0e6ae55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b0f9cbb2dd4f35aec6dbd6ec2682bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb356443b894c02829c7fa9be65c985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Ready — embedding dim: 384\n",
      "[embed_corpus] Embedding 8858 texts with sbert...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dd6ead147d4f8ea924299cdf1acbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed_corpus] Done. Shape: (8858, 384)\n",
      "[embed_corpus] Saved to ../data/processed/embeddings_sbert_minilm.npy\n"
     ]
    }
   ],
   "source": [
    "# SBERT MiniLM (English, fast)\n",
    "sbert_minilm = load_sbert('minilm')\n",
    "emb_minilm = embed_corpus(\n",
    "    texts=list(df_cnhpsx['headline_clean']),\n",
    "    method='sbert',\n",
    "    model=sbert_minilm,\n",
    "    save_path='../data/processed/embeddings_sbert_minilm.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Loading: all-mpnet-base-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cd0226b4ed49b89293abe158b2f20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38947e26da8645689b0ea25547744711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcec4f8f6fb14ee68f8012704bfb57d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de78576892b44d8e8bb3f8219e3bd649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54993d249f27484d91a059599037bea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ef62aa19e54f0fb8af4ee8f368803a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c760be46cc45e6bc76d20beea3f649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Ready — embedding dim: 768\n",
      "[embed_corpus] Embedding 8858 texts with sbert...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb02b564d5b46ca9de47b8b9370eade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed_corpus] Done. Shape: (8858, 768)\n",
      "[embed_corpus] Saved to ../data/processed/embeddings_sbert_mpnet.npy\n"
     ]
    }
   ],
   "source": [
    "# SBERT MPNet (English, best quality)\n",
    "sbert_mpnet = load_sbert('mpnet')\n",
    "emb_mpnet = embed_corpus(\n",
    "    texts=list(df_cnhpsx['headline_clean']),\n",
    "    method='sbert',\n",
    "    model=sbert_mpnet,\n",
    "    save_path='../data/processed/embeddings_sbert_mpnet.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison — cosine similarity on test headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks: (813588, 10)\n",
      "News: (25912, 5)\n",
      "CNH-PSX: (8858, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload dataframes that are no longer in memory\n",
    "df_stocks = pd.read_csv('../data/processed/psx_stocks_clean.csv')\n",
    "df_news   = pd.read_csv('../data/processed/pakistan_news_clean.csv')\n",
    "df_cnhpsx = pd.read_csv('../data/processed/cnhpsx_clean.csv')\n",
    "\n",
    "print('Stocks:', df_stocks.shape)\n",
    "print('News:', df_news.shape)\n",
    "print('CNH-PSX:', df_cnhpsx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Loading: paraphrase-multilingual-MiniLM-L12-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d439441bf6443ad8bb598808cca443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Ready — embedding dim: 384\n",
      "[SBERT] Loading: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ca9a6f9eb340939153a2a6b9158e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Ready — embedding dim: 384\n",
      "[SBERT] Loading: all-mpnet-base-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17115c0a06b94efb92e5bc7d33eeb15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBERT] Ready — embedding dim: 768\n",
      "[load_embeddings] Loaded (8858, 100) from ../data/processed/embeddings_w2v.npy\n",
      "[load_embeddings] Loaded (8858, 384) from ../data/processed/embeddings_sbert_multilingual.npy\n",
      "[load_embeddings] Loaded (8858, 384) from ../data/processed/embeddings_sbert_minilm.npy\n",
      "[load_embeddings] Loaded (8858, 768) from ../data/processed/embeddings_sbert_mpnet.npy\n",
      "All embeddings loaded!\n"
     ]
    }
   ],
   "source": [
    "from embeddings import load_embeddings, load_word2vec, embed_corpus\n",
    "# Reload the SBERT models into memory\n",
    "sbert_multi  = load_sbert('multilingual')\n",
    "sbert_minilm = load_sbert('minilm')\n",
    "sbert_mpnet  = load_sbert('mpnet')\n",
    "# Load all saved embeddings\n",
    "w2v_embeddings = load_embeddings('../data/processed/embeddings_w2v.npy')\n",
    "emb_multi      = load_embeddings('../data/processed/embeddings_sbert_multilingual.npy')\n",
    "emb_minilm     = load_embeddings('../data/processed/embeddings_sbert_minilm.npy')\n",
    "emb_mpnet      = load_embeddings('../data/processed/embeddings_sbert_mpnet.npy')\n",
    "\n",
    "print(\"All embeddings loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 1: CNH-PSX headlines ===\n",
      "A (Market): ['KSE index plunges by 83 points']\n",
      "B (Market): ['Karachi stocks record mixed trend,,,,,By our correspondent']\n",
      "C (Other):  ['Oil prices fall']\n",
      "\n",
      "=== TEST 2: Pakistan News (section='Pakistan') ===\n",
      "D (same section): Chinese national held for beating traffic police constable in Karachi\n",
      "E (same section): Sarmad Khoosat reveals why Zindagi Tamasha's trailer was removed from YouTube\n",
      "F (diff section): Iraqi paramilitaries call for withdrawal from US embassy\n",
      "\n",
      "=== TEST 3: PSX Stocks — ticker 'ABL' in headlines ===\n",
      "G (mentions ticker): ['Bulls remain on drive, KSE gains 118 points', 'KSE to be more profitable in 2007, says report']\n",
      "H (mentions ticker): ['KSE dips as investors book profit on available margins']\n",
      "I (other headline):  ['KSE index plunges by 83 points']\n",
      "\n",
      "\n",
      "=== FULL COMPARISON ACROSS ALL DATASETS ===\n",
      "                             A-B (same)  A-C (diff)       Δ\n",
      "[CNH] Word2Vec (clean)           0.8135      0.5888  0.2248\n",
      "[CNH] SBERT-Multi (clean)        0.2294      0.2065  0.0230\n",
      "[CNH] SBERT-Multi (raw)          0.2751      0.3147 -0.0397\n",
      "[CNH] SBERT-MiniLM (clean)       0.2994      0.2682  0.0312\n",
      "[CNH] SBERT-MiniLM (raw)         0.4256      0.3558  0.0697\n",
      "[CNH] SBERT-MPNet (clean)        0.3387      0.3328  0.0059\n",
      "[CNH] SBERT-MPNet (raw)          0.4185      0.4235 -0.0050\n",
      "[News] SBERT-Multi (raw)        -0.0215     -0.1026  0.0810\n",
      "[News] SBERT-MiniLM (raw)        0.0575     -0.0195  0.0770\n",
      "[News] SBERT-MPNet (raw)         0.0668     -0.0103  0.0772\n",
      "[Stocks] SBERT-Multi (raw)       0.4744      0.4122  0.0622\n",
      "[Stocks] SBERT-MiniLM (raw)      0.6056      0.5786  0.0269\n",
      "[Stocks] SBERT-MPNet (raw)       0.7068      0.5967  0.1101\n",
      "\n",
      "→ Best model overall: highest Δ\n",
      "→ Winner: [CNH] Word2Vec (clean) with Δ = 0.2248000055551529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from embeddings import get_sbert_embedding, get_word2vec_embedding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TEST 1 — CNH-PSX: same category vs different\n",
    "# ═══════════════════════════════════════════\n",
    "\n",
    "idx_a = df_cnhpsx[df_cnhpsx['category'] == 'Market'].index[0]\n",
    "idx_b = df_cnhpsx[df_cnhpsx['category'] == 'Market'].index[1]\n",
    "idx_c = df_cnhpsx[df_cnhpsx['category'] != 'Market'].index[0]\n",
    "\n",
    "print(\"=== TEST 1: CNH-PSX headlines ===\")\n",
    "print('A (Market):', df_cnhpsx.loc[idx_a, 'headline'])\n",
    "print('B (Market):', df_cnhpsx.loc[idx_b, 'headline'])\n",
    "print('C (Other): ', df_cnhpsx.loc[idx_c, 'headline'])\n",
    "\n",
    "# Word2Vec\n",
    "a, b, c = w2v_embeddings[idx_a], w2v_embeddings[idx_b], w2v_embeddings[idx_c]\n",
    "results['[CNH] Word2Vec (clean)'] = {\n",
    "    'A-B (same)': round(cosine_similarity([a], [b])[0][0], 4),\n",
    "    'A-C (diff)': round(cosine_similarity([a], [c])[0][0], 4),\n",
    "    'Δ': round(cosine_similarity([a], [b])[0][0] - cosine_similarity([a], [c])[0][0], 4)\n",
    "}\n",
    "\n",
    "for name, model, emb in [\n",
    "    ('SBERT-Multi',  sbert_multi,  emb_multi),\n",
    "    ('SBERT-MiniLM', sbert_minilm, emb_minilm),\n",
    "    ('SBERT-MPNet',  sbert_mpnet,  emb_mpnet),\n",
    "]:\n",
    "    # Clean\n",
    "    a_c, b_c, c_c = emb[idx_a], emb[idx_b], emb[idx_c]\n",
    "    sim_ab = cosine_similarity([a_c], [b_c])[0][0]\n",
    "    sim_ac = cosine_similarity([a_c], [c_c])[0][0]\n",
    "    results[f'[CNH] {name} (clean)'] = {\n",
    "        'A-B (same)': round(sim_ab, 4),\n",
    "        'A-C (diff)': round(sim_ac, 4),\n",
    "        'Δ': round(sim_ab - sim_ac, 4)\n",
    "    }\n",
    "    # Raw\n",
    "    a_r = get_sbert_embedding(df_cnhpsx.loc[idx_a, 'headline'], model)\n",
    "    b_r = get_sbert_embedding(df_cnhpsx.loc[idx_b, 'headline'], model)\n",
    "    c_r = get_sbert_embedding(df_cnhpsx.loc[idx_c, 'headline'], model)\n",
    "    sim_ab_r = cosine_similarity([a_r], [b_r])[0][0]\n",
    "    sim_ac_r = cosine_similarity([a_r], [c_r])[0][0]\n",
    "    results[f'[CNH] {name} (raw)'] = {\n",
    "        'A-B (same)': round(sim_ab_r, 4),\n",
    "        'A-C (diff)': round(sim_ac_r, 4),\n",
    "        'Δ': round(sim_ab_r - sim_ac_r, 4)\n",
    "    }\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TEST 2 — Pakistan News: same section vs different\n",
    "# ═══════════════════════════════════════════\n",
    "\n",
    "# Pick a section with enough articles\n",
    "top_section = df_news['section'].value_counts().index[0]\n",
    "idx_d = df_news[df_news['section'] == top_section].index[0]\n",
    "idx_e = df_news[df_news['section'] == top_section].index[1]\n",
    "idx_f = df_news[df_news['section'] != top_section].dropna(subset=['section']).index[0]\n",
    "\n",
    "print(f\"\\n=== TEST 2: Pakistan News (section='{top_section}') ===\")\n",
    "print('D (same section):', df_news.loc[idx_d, 'heading'])\n",
    "print('E (same section):', df_news.loc[idx_e, 'heading'])\n",
    "print('F (diff section):', df_news.loc[idx_f, 'heading'])\n",
    "\n",
    "for name, model in [\n",
    "    ('SBERT-Multi',  sbert_multi),\n",
    "    ('SBERT-MiniLM', sbert_minilm),\n",
    "    ('SBERT-MPNet',  sbert_mpnet),\n",
    "]:\n",
    "    # Raw headlines (Pakistan News has full text, no need for cleaned version)\n",
    "    d_r = get_sbert_embedding(df_news.loc[idx_d, 'text_combined'], model)\n",
    "    e_r = get_sbert_embedding(df_news.loc[idx_e, 'text_combined'], model)\n",
    "    f_r = get_sbert_embedding(df_news.loc[idx_f, 'text_combined'], model)\n",
    "    sim_de = cosine_similarity([d_r], [e_r])[0][0]\n",
    "    sim_df = cosine_similarity([d_r], [f_r])[0][0]\n",
    "    results[f'[News] {name} (raw)'] = {\n",
    "        'A-B (same)': round(sim_de, 4),\n",
    "        'A-C (diff)': round(sim_df, 4),\n",
    "        'Δ': round(sim_de - sim_df, 4)\n",
    "    }\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# TEST 3 — PSX Stocks: same ticker name in headline vs different\n",
    "# ═══════════════════════════════════════════\n",
    "\n",
    "# Find a ticker that appears in CNH-PSX headlines\n",
    "common_tickers = df_stocks['symbol'].unique()\n",
    "ticker_found = None\n",
    "for ticker in common_tickers:\n",
    "    mask = df_cnhpsx['headline'].str.contains(ticker, case=False, na=False)\n",
    "    if mask.sum() >= 2:\n",
    "        ticker_found = ticker\n",
    "        break\n",
    "\n",
    "if ticker_found:\n",
    "    ticker_headlines = df_cnhpsx[df_cnhpsx['headline'].str.contains(ticker_found, case=False, na=False)]\n",
    "    other_headlines  = df_cnhpsx[~df_cnhpsx['headline'].str.contains(ticker_found, case=False, na=False)]\n",
    "\n",
    "    idx_g = ticker_headlines.index[0]\n",
    "    idx_h = ticker_headlines.index[1]\n",
    "    idx_i = other_headlines.index[0]\n",
    "\n",
    "    print(f\"\\n=== TEST 3: PSX Stocks — ticker '{ticker_found}' in headlines ===\")\n",
    "    print('G (mentions ticker):', df_cnhpsx.loc[idx_g, 'headline'])\n",
    "    print('H (mentions ticker):', df_cnhpsx.loc[idx_h, 'headline'])\n",
    "    print('I (other headline): ', df_cnhpsx.loc[idx_i, 'headline'])\n",
    "\n",
    "    for name, model in [\n",
    "        ('SBERT-Multi',  sbert_multi),\n",
    "        ('SBERT-MiniLM', sbert_minilm),\n",
    "        ('SBERT-MPNet',  sbert_mpnet),\n",
    "    ]:\n",
    "        g_r = get_sbert_embedding(df_cnhpsx.loc[idx_g, 'headline'], model)\n",
    "        h_r = get_sbert_embedding(df_cnhpsx.loc[idx_h, 'headline'], model)\n",
    "        i_r = get_sbert_embedding(df_cnhpsx.loc[idx_i, 'headline'], model)\n",
    "        sim_gh = cosine_similarity([g_r], [h_r])[0][0]\n",
    "        sim_gi = cosine_similarity([g_r], [i_r])[0][0]\n",
    "        results[f'[Stocks] {name} (raw)'] = {\n",
    "            'A-B (same)': round(sim_gh, 4),\n",
    "            'A-C (diff)': round(sim_gi, 4),\n",
    "            'Δ': round(sim_gh - sim_gi, 4)\n",
    "        }\n",
    "else:\n",
    "    print(\"\\nNo common ticker found between stocks and headlines — skipping test 3\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════\n",
    "# Final results table\n",
    "# ═══════════════════════════════════════════\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print('\\n\\n=== FULL COMPARISON ACROSS ALL DATASETS ===')\n",
    "print(df_results.to_string())\n",
    "print('\\n→ Best model overall: highest Δ')\n",
    "best = df_results['Δ'].idxmax()\n",
    "print(f'→ Winner: {best} with Δ = {df_results.loc[best, \"Δ\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
